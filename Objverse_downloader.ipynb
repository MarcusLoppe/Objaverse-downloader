{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Step 1 - Get model sizes & path\n","\n","Option 1 - Extract manually:\n","1. Run \"git clone https://huggingface.co/datasets/allenai/objaverse\" and then abort the command when it starts to download the models.\n","2. This will create a git repo folder, you then can run \"python dump_gitcommits.py > out.txt\" to dump the entire commit history\n","3. Then you call extract_models_from_dump(\"out.txt\") to parse and get all the model paths and their sizes.\n","\n","Option 2 - Use the pre-extracted json (model_sizes.json.gz)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os,json\n","import requests\n","import gzip\n","\n","def extract_models_from_dump(file_path):\n","    model_sizes = {}\n","    current_model = None\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            # Check if line contains \".glb\"\n","            if \".glb\" in line:\n","                # Extract model path\n","                model_path = line.split()[-1].strip()\n","                # Remove leading \"b\" if present\n","                model_path = model_path.replace(\"b/\", \"\")\n","                current_model = model_path\n","            # Check if line contains \"size\" and current_model is set\n","            elif current_model and \"size\" in line:\n","                # Extract size\n","                size = int(line.split()[-1].strip())\n","                # Store model size\n","                model_sizes[current_model] = size\n","                # Reset current_model for next iteration\n","                current_model = None\n","    return model_sizes\n"," \n"," \n"," ## Option 1\n","#model_sizes = extract_models_from_dump(\"out.txt\")  \n","\n","\n","## Option 2\n","with gzip.open(\"model_sizes.json.gz\", 'rb') as gzip_file: \n","    model_sizes = json.loads(gzip_file.read().decode('utf-8'))\n","    \n","print(len(model_sizes))"]},{"cell_type":"markdown","metadata":{},"source":["### Download the meshes as per specified size limit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","from tqdm import tqdm  \n","from concurrent.futures import ThreadPoolExecutor \n","\n","def download_model(model_url, save_path):\n","    try:\n","        response = requests.get(model_url)\n","        if response.status_code == 200:\n","            with open(save_path, 'wb') as f:\n","                f.write(response.content)\n","                #print(f\"Downloaded: {save_path}\")\n","        else:\n","            print(f\"Failed to download: {model_url}\")\n","    except Exception as e:\n","        print(f\"Error downloading: {model_url}, {e}\")\n","\n","def download_filtered_models(model_sizes, base_url, save_dir, minKb, maxKb,num_threads = 6):\n","    filtered_models = {model_path: size for model_path, size in model_sizes.items() if minKb < size < maxKb * 1024}\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        futures = []\n","        for model_path, size in filtered_models.items():\n","            folder_name = os.path.dirname(model_path)\n","            sub_folder = os.path.join(save_dir, folder_name)\n","            os.makedirs(sub_folder, exist_ok=True)\n","            file_name = os.path.basename(model_path)\n","            save_path = os.path.join(sub_folder, file_name)\n","            if not os.path.exists(save_path):\n","                model_url = f\"{base_url}/{model_path}?download=true\"\n","                futures.append(executor.submit(download_model, model_url, save_path))\n","        for future in tqdm(futures, total=len(futures)):\n","            future.result()\n","            \n","base_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main\"  \n","save_dir = f'./objaverse' \n","\n","os.makedirs(save_dir, exist_ok=True)   \n","download_filtered_models(model_sizes, base_url, save_dir, minKb = 2, maxKb = 80, num_threads= 6) "]},{"cell_type":"markdown","metadata":{},"source":["### Download metadata"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm\n","\n","# Function to download files\n","def download_file(url, folder_path, filename):\n","    url = url + \"?download=true\"\n","    print(url)\n","    response = requests.get(url, stream=True)\n","    if response.status_code == 200:\n","        with open(os.path.join(folder_path, filename), 'wb') as f:\n","            f.write(response.content)\n","        #print(f\"Downloaded {filename}\")\n","    else:\n","        print(f\"Failed to download {filename}\")\n"," \n","def download_metadata(base_url, save_dir,  num_threads=6):\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        futures = []\n","        for i in range(1, 161):\n","            filename = f\"000-{i:03d}.json.gz\"\n","            file_url = base_url + filename\n","            futures.append(executor.submit(download_file, file_url, save_dir, filename))\n","        \n","        # Wait for all tasks to complete\n","        for future in tqdm(futures, total=len(futures)):\n","            future.result()\n","\n","# Example usage\n","base_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/\" \n","save_dir = './objaverse/metadata'\n","os.makedirs(save_dir, exist_ok=True)   \n","\n","download_metadata(base_url, save_dir)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Extract the metadata to a JSON with only the relevant information, e.g the models you downloaded"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","\n","directory = './objaverse/glbs'  \n","\n","existing_models = {}\n","for file_path in glob.iglob(directory + '/**/*', recursive=True):\n","    if os.path.isfile(file_path):\n","        file_name, file_extension = os.path.splitext(file_path)\n","        existing_models[os.path.basename(file_name)] = file_path\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import os,gzip,json\n","metadata = {}\n","filtered_metadata = { }\n","\n","\n","metadata_path = './objaverse/metadata'\n","for file_name in os.listdir(metadata_path):\n","    if file_name.endswith(\".gz\"): \n","        file_path = os.path.join(metadata_path, file_name) \n","        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n","            metadata = json.load(f)\n","            for key, value in existing_models.items():\n","                if key in metadata:\n","                    filtered_metadata[key] = metadata[key] \n","        "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["with open('./objaverse/metadata.json'  , 'w') as f:\n","    json.dump(filtered_metadata, f)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4161603,"sourceId":7195861,"sourceType":"datasetVersion"},{"datasetId":4161651,"sourceId":7195931,"sourceType":"datasetVersion"},{"datasetId":4161685,"sourceId":7195983,"sourceType":"datasetVersion"},{"datasetId":4161916,"sourceId":7196324,"sourceType":"datasetVersion"},{"datasetId":4162608,"sourceId":7197423,"sourceType":"datasetVersion"},{"datasetId":4166703,"sourceId":7202809,"sourceType":"datasetVersion"},{"datasetId":4166781,"sourceId":7202905,"sourceType":"datasetVersion"},{"datasetId":4166927,"sourceId":7203105,"sourceType":"datasetVersion"},{"datasetId":4167078,"sourceId":7203339,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
